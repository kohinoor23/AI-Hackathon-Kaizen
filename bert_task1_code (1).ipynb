{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":72549,"databundleVersionId":7959527,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q tqdm boto3 requests regex sentencepiece sacremoses","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-17T10:01:22.991837Z","iopub.execute_input":"2024-03-17T10:01:22.992664Z","iopub.status.idle":"2024-03-17T10:01:41.086184Z","shell.execute_reply.started":"2024-03-17T10:01:22.992629Z","shell.execute_reply":"2024-03-17T10:01:41.085080Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.11.2 requires botocore<1.34.35,>=1.33.2, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')    # Download vocabulary from S3 and cache.\nbertmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased').to(device)\n# Download model and configuration from S3 and cache.","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:56:47.137930Z","iopub.execute_input":"2024-03-17T10:56:47.138315Z","iopub.status.idle":"2024-03-17T10:56:54.636549Z","shell.execute_reply.started":"2024-03-17T10:56:47.138284Z","shell.execute_reply":"2024-03-17T10:56:54.635232Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n  warnings.warn(\nDownloading: \"https://github.com/huggingface/pytorch-transformers/zipball/main\" to /root/.cache/torch/hub/main.zip\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuggingface/pytorch-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# Download vocabulary from S3 and cache.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m bertmodel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/pytorch-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Download model and configuration from S3 and cache.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/hub.py:566\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    563\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    564\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 566\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/hub.py:594\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m    592\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m--> 594\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[43m_load_entry_from_hubconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhub_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/hub.py:343\u001b[0m, in \u001b[0;36m_load_entry_from_hubconf\u001b[0;34m(m, model)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid input: model should be a string of function name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Note that if a missing dependency is imported at top level of hubconf, it will\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# throw before this function. It's a chicken and egg situation where we have to\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# load hubconf to know what're the dependencies, but to import hubconf it requires\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# a missing package. This is fine, Python will throw proper error message for users.\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[43m_check_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m func \u001b[38;5;241m=\u001b[39m _load_attr_from_module(m, model)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/hub.py:332\u001b[0m, in \u001b[0;36m_check_dependencies\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m    330\u001b[0m missing_deps \u001b[38;5;241m=\u001b[39m [pkg \u001b[38;5;28;01mfor\u001b[39;00m pkg \u001b[38;5;129;01min\u001b[39;00m dependencies \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _check_module_exists(pkg)]\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_deps):\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing dependencies: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_deps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: Missing dependencies: sacremoses"],"ename":"RuntimeError","evalue":"Missing dependencies: sacremoses","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/ai-for-social-good-aries-iitd-x-kaizen-24/Public/Task 1/train.csv')\ndftest = pd.read_csv('/kaggle/input/ai-for-social-good-aries-iitd-x-kaizen-24/Public/Task 1/dev.csv')\ndfval = pd.read_csv('/kaggle/input/ai-for-social-good-aries-iitd-x-kaizen-24/Public/Task 1/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:01:55.748433Z","iopub.execute_input":"2024-03-17T10:01:55.749338Z","iopub.status.idle":"2024-03-17T10:01:56.285285Z","shell.execute_reply.started":"2024-03-17T10:01:55.749309Z","shell.execute_reply":"2024-03-17T10:01:56.284311Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"zerocolumns = df[df['claim']==0]\nonecolumns = df[df['claim']==1].sample(frac=1)[:len(zerocolumns)]\ndfequal = pd.concat([zerocolumns, onecolumns]).sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:01:56.287779Z","iopub.execute_input":"2024-03-17T10:01:56.288358Z","iopub.status.idle":"2024-03-17T10:01:56.306762Z","shell.execute_reply.started":"2024-03-17T10:01:56.288325Z","shell.execute_reply":"2024-03-17T10:01:56.305807Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"x_train = []\ny_train = []\n\nwith torch.no_grad():\n    for i in range(len(dfequal)):\n    # for i in range(100):\n        tweet = dfequal['tweet_text'][i]\n        tokens = tokenizer.encode(tweet, add_special_tokens=True)\n        tokens_tensor = torch.tensor([tokens]).to(device)\n        clsembed = bertmodel(tokens_tensor)[0][0][0]\n        x_train.append(clsembed)\n        y_train.append(dfequal['claim'][i])\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:22:18.090043Z","iopub.execute_input":"2024-03-17T10:22:18.090417Z","iopub.status.idle":"2024-03-17T10:22:35.165009Z","shell.execute_reply.started":"2024-03-17T10:22:18.090387Z","shell.execute_reply":"2024-03-17T10:22:35.164221Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"x_train_comp = []\ny_train_comp = []\n\nwith torch.no_grad():\n    for i in range(2500):\n    # for i in range(100):\n        tweet = df['tweet_text'][i]\n        tokens = tokenizer.encode(tweet, add_special_tokens=True)\n        tokens_tensor = torch.tensor([tokens]).to(device)\n        clsembed = bertmodel(tokens_tensor)[0][0][0]\n        x_train_comp.append(clsembed)\n        y_train_comp.append(df['claim'][i])\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:27:54.935982Z","iopub.execute_input":"2024-03-17T10:27:54.936841Z","iopub.status.idle":"2024-03-17T10:28:19.197717Z","shell.execute_reply.started":"2024-03-17T10:27:54.936808Z","shell.execute_reply":"2024-03-17T10:28:19.196845Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"x_test = []\ny_test = []\n\nwith torch.no_grad():\n    for i in range(len(dftest)):\n    # for i in range(100):\n        tweet = dftest['tweet_text'][i]\n        tokens = tokenizer.encode(tweet, add_special_tokens=True)\n        tokens_tensor = torch.tensor([tokens]).to(device)\n        clsembed = bertmodel(tokens_tensor)[0][0][0]\n        x_test.append(clsembed)\n        y_test.append(dftest['claim'][i])","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:22:35.166426Z","iopub.execute_input":"2024-03-17T10:22:35.166712Z","iopub.status.idle":"2024-03-17T10:22:49.802119Z","shell.execute_reply.started":"2024-03-17T10:22:35.166687Z","shell.execute_reply":"2024-03-17T10:22:49.801076Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass BertClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(BertClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax()\n#         self.dropout = nn.Dropout(p=0.3)\n#         self.bn1 = nn.BatchNorm1d(input_size)\n    \n    def forward(self, x):\n#         x = self.bn1(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n#         x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \nmodel = BertClassifier(768, 256, 2).to(device)\n\ninput_size = 768\nhidden_size = 256\noutput_size = 1\nlearning_rate = 0.0005\nnum_epochs = 10\nbatch_size = 32\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# from torch.utils.data import DataLoader\n\n# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:57:43.537294Z","iopub.execute_input":"2024-03-17T10:57:43.537642Z","iopub.status.idle":"2024-03-17T10:57:43.549697Z","shell.execute_reply.started":"2024-03-17T10:57:43.537616Z","shell.execute_reply":"2024-03-17T10:57:43.548942Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:57:47.067074Z","iopub.execute_input":"2024-03-17T10:57:47.067454Z","iopub.status.idle":"2024-03-17T10:57:47.073814Z","shell.execute_reply.started":"2024-03-17T10:57:47.067423Z","shell.execute_reply":"2024-03-17T10:57:47.072907Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"BertClassifier(\n  (fc1): Linear(in_features=768, out_features=256, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=256, out_features=2, bias=True)\n  (softmax): Softmax(dim=None)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.autograd\nfrom sklearn.metrics import f1_score\n# Enable anomaly detection\ntorch.autograd.set_detect_anomaly(True)\n\nfor i in range(num_epochs):\n    print(i)\n    for i in range(0, len(x_train), batch_size):\n#         tweet = df['tweet_text'][i:i+batch_size]\n#         tokens = tokenizer.encode(tweet, add_special_tokens=True)\n#         tokens_tensor = torch.tensor([tokens]).to(device)\n#         clsembed = bertmodel(tokens_tensor)[0][0][0]\n#         print(clsembed)\n        inputs = torch.stack(x_train[i:i+batch_size]).to(device)\n        targets = torch.tensor(y_train[i:i+batch_size]).to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Compute loss\n        loss = criterion(outputs, targets)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(loss)\n    \n    with torch.no_grad():\n    # Forward pass\n        outputs = model(torch.stack(x_train).to(device))\n\n    predicted_labels = torch.argmax(outputs, axis=1)\n    print(sum(predicted_labels==0))\n    accuracy = f1_score(predicted_labels.cpu().numpy(), y_train)\n    print(f\"Accuracy on train={accuracy}\")\n    \n    with torch.no_grad():\n    # Forward pass on test\n        outputs = model(torch.stack(x_test).to(device))\n\n    predicted_labels = torch.argmax(outputs, axis=1)\n    print(sum(predicted_labels==0))\n    y_actual = torch.tensor(y_test).to(device)\n    accuracy = f1_score(predicted_labels.cpu().numpy(), y_test)\n    print(f\"Accuracy on test={accuracy}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:25:52.476236Z","iopub.execute_input":"2024-03-17T10:25:52.476610Z","iopub.status.idle":"2024-03-17T10:25:59.450642Z","shell.execute_reply.started":"2024-03-17T10:25:52.476582Z","shell.execute_reply":"2024-03-17T10:25:59.449782Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(740, device='cuda:0')\nAccuracy on train=0.6726315789473685\ntensor(616, device='cuda:0')\nAccuracy on test=0.7534372135655363\n1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(764, device='cuda:0')\nAccuracy on train=0.6940298507462686\ntensor(630, device='cuda:0')\nAccuracy on test=0.7518450184501846\n2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(791, device='cuda:0')\nAccuracy on train=0.709572742022715\ntensor(658, device='cuda:0')\nAccuracy on test=0.7429906542056075\n3\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(838, device='cuda:0')\nAccuracy on train=0.7180910099889012\ntensor(681, device='cuda:0')\nAccuracy on test=0.7321681624940954\n4\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(841, device='cuda:0')\nAccuracy on train=0.7292940522512505\ntensor(695, device='cuda:0')\nAccuracy on test=0.7256300523062291\n5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(839, device='cuda:0')\nAccuracy on train=0.7473625763464741\ntensor(707, device='cuda:0')\nAccuracy on test=0.7183165949306553\n6\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(853, device='cuda:0')\nAccuracy on train=0.7498601007274762\ntensor(699, device='cuda:0')\nAccuracy on test=0.7279656979514054\n7\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(870, device='cuda:0')\nAccuracy on train=0.7627118644067796\ntensor(706, device='cuda:0')\nAccuracy on test=0.7246653919694073\n8\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(859, device='cuda:0')\nAccuracy on train=0.768107804604155\ntensor(708, device='cuda:0')\nAccuracy on test=0.7196172248803826\n9\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(870, device='cuda:0')\nAccuracy on train=0.7751412429378532\ntensor(699, device='cuda:0')\nAccuracy on test=0.7279656979514054\n10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5442, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(879, device='cuda:0')\nAccuracy on train=0.7836456558773424\ntensor(698, device='cuda:0')\nAccuracy on test=0.7228571428571429\n11\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5373, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(871, device='cuda:0')\nAccuracy on train=0.7925381571509328\ntensor(702, device='cuda:0')\nAccuracy on test=0.7251908396946566\n12\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5193, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(890, device='cuda:0')\nAccuracy on train=0.8011428571428572\ntensor(708, device='cuda:0')\nAccuracy on test=0.7224880382775118\n13\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5219, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(874, device='cuda:0')\nAccuracy on train=0.8040770101925254\ntensor(708, device='cuda:0')\nAccuracy on test=0.724401913875598\n14\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(887, device='cuda:0')\nAccuracy on train=0.8123217341699942\ntensor(709, device='cuda:0')\nAccuracy on test=0.7209191000478697\n15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5045, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(877, device='cuda:0')\nAccuracy on train=0.8167895632444697\ntensor(705, device='cuda:0')\nAccuracy on test=0.7204968944099379\n16\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5009, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(873, device='cuda:0')\nAccuracy on train=0.822863610639502\ntensor(704, device='cuda:0')\nAccuracy on test=0.7211079274116523\n17\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.5117, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(877, device='cuda:0')\nAccuracy on train=0.8315371525808282\ntensor(700, device='cuda:0')\nAccuracy on test=0.7235462345090562\n18\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(874, device='cuda:0')\nAccuracy on train=0.8267270668176671\ntensor(693, device='cuda:0')\nAccuracy on test=0.7287410926365794\n19\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.4978, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(868, device='cuda:0')\nAccuracy on train=0.8442437923250564\ntensor(683, device='cuda:0')\nAccuracy on test=0.7328605200945626\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Testing accuracy\nwith torch.no_grad():\n    # Forward pass\n    outputs = model(torch.stack(x_train).to(device))\n\npredicted_labels = torch.argmax(outputs, axis=1)\nprint(sum(predicted_labels==0))\naccuracy = (predicted_labels == torch.tensor(y_train).to(device)).float().mean()\nprint(f\"Accuracy={accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:30:21.149099Z","iopub.execute_input":"2024-03-17T10:30:21.149828Z","iopub.status.idle":"2024-03-17T10:30:21.179186Z","shell.execute_reply.started":"2024-03-17T10:30:21.149795Z","shell.execute_reply":"2024-03-17T10:30:21.178279Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"tensor(872, device='cuda:0')\nAccuracy=0.8352272510528564\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"# from torcheval.metrics.functional import multiclass_f1_score\n\nwith torch.no_grad():\n    # Forward pass\n    outputs = model(torch.stack(x_test).to(device))\n\npredicted_labels = torch.argmax(outputs, axis=1)\nprint(sum(predicted_labels==0))\ny_actual = torch.tensor(y_test).to(device)\naccuracy = (predicted_labels == y_actual).float().mean()\nprint(f\"Accuracy={accuracy}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T07:00:38.635996Z","iopub.execute_input":"2024-03-17T07:00:38.636342Z","iopub.status.idle":"2024-03-17T07:00:38.655343Z","shell.execute_reply.started":"2024-03-17T07:00:38.636315Z","shell.execute_reply":"2024-03-17T07:00:38.654486Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"tensor(347, device='cuda:0')\nAccuracy=0.7020000219345093\n","output_type":"stream"}]},{"cell_type":"code","source":"cur_model = model\n\ndf_submit = pd.read_csv('/kaggle/input/ai-for-social-good-aries-iitd-x-kaizen-24/Public/submission_format.csv')\ndf_submit = df_submit[df_submit['task']==1]\nx_submit = []\ny_submit = []\n\nwith torch.no_grad():\n    for i in range(len(df_submit)):\n    # for i in range(100):\n        tweet = df_submit['text'][i]\n        tokens = tokenizer.encode(tweet, add_special_tokens=True)\n        tokens_tensor = torch.tensor([tokens]).to(device)\n        clsembed = bertmodel(tokens_tensor)[0][0][0]\n        x_submit.append(clsembed)\n        y_submit.append(df_submit['claim'][i])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:15:36.243765Z","iopub.execute_input":"2024-03-17T10:15:36.244193Z","iopub.status.idle":"2024-03-17T10:15:50.870808Z","shell.execute_reply.started":"2024-03-17T10:15:36.244146Z","shell.execute_reply":"2024-03-17T10:15:50.869803Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    # Forward pass\n    outputs = model(torch.stack(x_submit).to(device))\npredicted_labels = torch.argmax(outputs, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:16:15.555305Z","iopub.execute_input":"2024-03-17T10:16:15.555681Z","iopub.status.idle":"2024-03-17T10:16:15.563100Z","shell.execute_reply.started":"2024-03-17T10:16:15.555649Z","shell.execute_reply":"2024-03-17T10:16:15.562080Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_te = pd.read_csv('/kaggle/input/ai-for-social-good-aries-iitd-x-kaizen-24/Public/submission_format.csv')\nlabels = predicted_labels.to('cpu')\n# df_te[df_te['task']==1]['claim'] = labels.numpy().copy()\ndf_te.loc[df_te['task']==1, 'claim'] = labels.numpy()","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:16:18.590539Z","iopub.execute_input":"2024-03-17T10:16:18.590912Z","iopub.status.idle":"2024-03-17T10:16:18.613145Z","shell.execute_reply.started":"2024-03-17T10:16:18.590880Z","shell.execute_reply":"2024-03-17T10:16:18.612414Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df_te.to_csv('ans.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:16:23.827228Z","iopub.execute_input":"2024-03-17T10:16:23.827628Z","iopub.status.idle":"2024-03-17T10:16:23.861942Z","shell.execute_reply.started":"2024-03-17T10:16:23.827598Z","shell.execute_reply":"2024-03-17T10:16:23.861183Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df_te[df_te['task']==1]['claim']","metadata":{"execution":{"iopub.status.busy":"2024-03-17T10:16:30.154786Z","iopub.execute_input":"2024-03-17T10:16:30.155518Z","iopub.status.idle":"2024-03-17T10:16:30.165070Z","shell.execute_reply.started":"2024-03-17T10:16:30.155488Z","shell.execute_reply":"2024-03-17T10:16:30.164067Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0       1\n1       0\n2       1\n3       0\n4       1\n       ..\n1493    1\n1494    0\n1495    1\n1496    1\n1497    0\nName: claim, Length: 1498, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}